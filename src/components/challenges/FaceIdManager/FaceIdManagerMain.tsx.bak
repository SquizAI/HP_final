import React, { useState, useRef, useEffect, useCallback } from 'react'
import { CheckCircle, XCircle, User, Shield, Sun, Image as ImageIcon, InfoIcon, RefreshCw, AlertTriangle } from 'lucide-react'
import ChallengeHeader from '../../shared/ChallengeHeader'
import { useUserProgress, markChallengeAsCompleted } from '../../../utils/userDataManager'
import { Scan } from 'lucide-react'

// Sample data for simulating face recognition
const registeredFaces = [
  { 
    id: 1, 
    name: 'Jordan Wilson', 
    role: 'IT Administrator', 
    confidence: 0.97, 
    authorized: true, 
    lastSeen: 'Today', 
    imageUrl: '/samples/sample-face-1.jpg' 
  },
  { id: 2, name: 'Taylor Rodriguez', role: 'Software Engineer', confidence: 0.98, authorized: true, lastSeen: '1 hour ago', imageUrl: '/samples/face-2.jpg' },
  { id: 3, name: 'Morgan Chen', role: 'Product Manager', confidence: 0.95, authorized: true, lastSeen: 'Today', imageUrl: '/samples/face-3.jpg' },
  { id: 4, name: 'Jamie Smith', role: 'UX Designer', confidence: 0.94, authorized: false, lastSeen: '3 days ago', imageUrl: '/samples/face-4.jpg' },
  { id: 5, name: 'Casey Williams', role: 'Data Scientist', confidence: 0.96, authorized: true, lastSeen: 'Today', imageUrl: '/samples/face-5.jpg' },
  { id: 6, name: 'Alex Johnson', role: 'HR Manager', confidence: 0.93, authorized: true, lastSeen: '2 days ago', imageUrl: '/samples/sample-face-6.jpg' },
  { id: 7, name: 'Riley Kumar', role: 'Marketing Director', confidence: 0.97, authorized: true, lastSeen: 'Yesterday', imageUrl: '/samples/sample-face-7.jpg' },
  { id: 8, name: 'Sam Washington', role: 'Finance Analyst', confidence: 0.92, authorized: false, lastSeen: '1 week ago', imageUrl: '/samples/sample-face-8.jpg' },
  { id: 9, name: 'Drew Garcia', role: 'Operations Manager', confidence: 0.95, authorized: true, lastSeen: 'Today', imageUrl: '/samples/sample-face-9.jpg' },
  { id: 10, name: 'Quinn Patel', role: 'Security Officer', confidence: 0.98, authorized: true, lastSeen: '3 hours ago', imageUrl: '/samples/sample-face-10.jpg' }
]

// Sample factors that can affect recognition accuracy
const environmentalFactors = [
  { name: 'Lighting', icon: <Sun size={20} />, description: 'Poor lighting can reduce accuracy by up to 30%.' },
  { name: 'Angle', icon: <ImageIcon size={20} />, description: 'Face angles exceeding 15Â° from center can decrease recognition rates.' },
  { name: 'Changes in Appearance', icon: <User size={20} />, description: 'Glasses, masks, or hairstyle changes may require system retraining.' },
]

// Sample business use cases for real-world uses
const businessUses = [
  { name: 'Smartphone Security', description: 'Apple Face ID and Android face unlock replace passwords with secure 3D facial mapping.' },
  { name: 'Airport Check-Ins', description: 'AI matches travelers to passport photos, speeding up security and immigration.' },
  { name: 'Banking & Payments', description: 'Facial verification adds extra security for account access and transactions.' },
  { name: 'Retail & Customer Experience', description: 'Stores personalize service and analyze shopper trends using facial recognition.' },
  { name: 'Healthcare', description: 'AI helps identify patients, assess pain levels, and assist in diagnosing genetic conditions.' },
]

// Sample privacy considerations
const privacyConsiderations = [
  { name: 'Secure Data Handling', description: 'Facial recognition systems should store data securely, using encryption and local processing where possible to protect privacy.' },
  { name: 'Consent & Transparency', description: 'Ethical AI should always require clear user consent before collecting facial data and provide information on how it will be used, stored, and shared.' },
  { name: 'Avoiding Bias', description: 'AI models can sometimes struggle with accuracy across different skin tones, genders, or ages. Ensuring fairness requires diverse training data and ongoing testing.' },
  { name: 'Thoughtful Use', description: 'Facial recognition should be applied only where necessary, balancing security benefits with privacy rights and ethical considerations.' },
  { name: 'Following Global Laws', description: 'Organizations must comply with privacy regulations like GDPR, CCPA, and other international standards to ensure responsible and legal use of biometric data.' },
]

// Add Window interface extension to properly type the faceapi global object
declare global {
  interface Window {
    faceapi: any;
    FaceDetector?: any; // Add support for the browser's built-in FaceDetector API
    connectCameraStream?: () => boolean; // Add this line
  }
}

const FaceIdManagerMain: React.FC = () => {
  const [uploadedImage, setUploadedImage] = useState<string | null>(null)
  const [demoStage, setDemoStage] = useState<'upload' | 'processing' | 'result'>('upload')
  const [recognitionResult, setRecognitionResult] = useState<typeof registeredFaces[0] | null>(null)
  const [showAdvancedOptions, setShowAdvancedOptions] = useState(false)
  const [confidenceThreshold, setConfidenceThreshold] = useState(0.9)
  const [isAuthorizedOnly, setIsAuthorizedOnly] = useState(true)
  const videoRef = useRef<HTMLVideoElement>(null)
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const [isModelLoaded, setIsModelLoaded] = useState(false)
  const [faceDescriptors, setFaceDescriptors] = useState<any[]>([])
  const [usingCamera, setUsingCamera] = useState(false)
  const [cameraError, setCameraError] = useState<string | null>(null)
  
  // Add state for challenge completion and confetti
  const [userProgress, setUserProgress] = useUserProgress();
  const [isCompleted, setIsCompleted] = useState<boolean>(
    userProgress.completedChallenges.includes('challenge-7')
  );
  const [showConfetti, setShowConfetti] = useState<boolean>(false);
  
  // Added state for user's captured face data
  const [userFaceDescriptor, setUserFaceDescriptor] = useState<any>(null);
  const [authenticatedUser, setAuthenticatedUser] = useState<typeof registeredFaces[0] | null>(null);
  const [isFaceDB, setIsFaceDB] = useState<boolean>(false);
  
  // Add new states for face registration
  const [registrationMode, setRegistrationMode] = useState<boolean>(false);
  const [registeredUserFace, setRegisteredUserFace] = useState<any>(null);
  const [registeredUserName, setRegisteredUserName] = useState<string>('');
  const [showRegistrationSuccess, setShowRegistrationSuccess] = useState<boolean>(false);
  
  // Add new state variables for local face detection
  const [useLocalDetection, setUseLocalDetection] = useState<boolean>(false);
  
  // Check if browser supports the FaceDetector API
  useEffect(() => {
    const checkBrowserFaceDetector = async () => {
      // Check if the browser supports the FaceDetector API
      if ('FaceDetector' in window) {
        try {
          // Try to create an instance to confirm it's actually available
          // @ts-ignore - FaceDetector might not be recognized by TypeScript
          const faceDetector = new window.FaceDetector();
          console.log('Browser FaceDetector API is available');
          // If it's available, use it by default
          setUseLocalDetection(true);
        } catch (err) {
          console.warn('Face Detector API is available but not working:', err);
          setUseLocalDetection(false);
        }
      } else {
        console.warn('Face Detector API is not available in this browser');
        setUseLocalDetection(false);
      }
    };
    
    checkBrowserFaceDetector();
  }, []);
  
  // Check if challenge is already completed on mount
  useEffect(() => {
    if (userProgress.completedChallenges.includes('challenge-7')) {
      setIsCompleted(true);
    }
  }, [userProgress]);
  
  // Load face-api.js models
  useEffect(() => {
    const loadFaceApiModels = async () => {
      try {
        // Check if Face-API script already exists
        const scriptExists = document.getElementById('face-api-script');
        if (!scriptExists) {
          // Load the script
          const script = document.createElement('script');
          script.id = 'face-api-script';
          // Use a more compatible version of face-api.js
          script.src = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.9/dist/face-api.js';
          script.async = true;
          script.onload = async () => {
            try {
              await loadModels();
            } catch (err) {
              console.error('Error loading models:', err);
              // Make sure we set a fallback state in case of errors
              setIsModelLoaded(false);
            }
          };
          document.head.appendChild(script);
        } else {
          // Script already exists, just load models
          await loadModels();
        }
      } catch (error) {
        console.error('Error initializing face-api:', error);
        // Enable simulation mode
        setIsModelLoaded(false);
      }
    };

    const loadModels = async () => {
      try {
        const modelPaths = [
          '/models',  // Local path
          'https://justadudewhohacks.github.io/face-api.js/models', // GitHub demo models
          'https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights' // GitHub raw models
        ];
        
        // Try loading from different paths until successful
        let success = false;
        
        for (const modelPath of modelPaths) {
          try {
            console.log(`Attempting to load models from ${modelPath}...`);
            // @ts-ignore - face-api is loaded from CDN
            await Promise.all([
              window.faceapi.nets.tinyFaceDetector.loadFromUri(modelPath),
              window.faceapi.nets.faceLandmark68Net.loadFromUri(modelPath),
              window.faceapi.nets.faceRecognitionNet.loadFromUri(modelPath)
              // Skip faceExpressionNet as it's not needed and might cause issues
            ]);
            console.log('Face-API models loaded successfully from', modelPath);
            success = true;
            break; // Exit loop on successful load
          } catch (e) {
            console.warn(`Failed to load models from ${modelPath}, trying next option...`);
            continue;
          }
        }
        
        if (success) {
          setIsModelLoaded(true);
          await loadRegisteredFaceDescriptors();
        } else {
          throw new Error('Could not load models from any source');
        }
      } catch (error) {
        console.error('Failed to load face-api.js models:', error);
        setIsModelLoaded(false);
      }
    };

    const loadRegisteredFaceDescriptors = async () => {
      try {
        const descriptors = [];
        // For each registered face, load the reference image and compute the descriptor
        for (const face of registeredFaces) {
          // Use a placeholder URL if the actual image isn't available
          const img = await createImageElement(face.imageUrl || `/samples/face-${face.id}.jpg`);
          if (img) {
            // @ts-ignore - face-api is loaded from CDN
            const detection = await window.faceapi.detectSingleFace(img, new window.faceapi.TinyFaceDetectorOptions({
              inputSize: 416, // Default: 416
              scoreThreshold: 0.5 // Default: 0.5
            }))
              // @ts-ignore - face-api is loaded from CDN
              .withFaceLandmarks()
              // @ts-ignore - face-api is loaded from CDN
              .withFaceDescriptor();
            
            if (detection) {
              descriptors.push({
                ...face,
                descriptor: detection.descriptor
              });
            }
          }
        }
        setFaceDescriptors(descriptors);
        setIsFaceDB(descriptors.length > 0);
        console.log('Registered face descriptors loaded:', descriptors.length);
      } catch (error) {
        console.error('Error loading face descriptors:', error);
      }
    };

    // Add a delay to give the browser some time before loading models
    setTimeout(() => {
      loadFaceApiModels();
    }, 1000);
    
    // Cleanup function to prevent memory leaks
    return () => {
      // If we need to clean up any face-api resources
      const script = document.getElementById('face-api-script');
      if (script) {
        // Just remove reference, don't actually remove the script as it might be used elsewhere
        script.id = 'face-api-script-used';
      }
    };
  }, []);

  // Load previously registered user face from localStorage
  useEffect(() => {
    const loadUserFace = () => {
      try {
        const savedUserFace = localStorage.getItem('registeredUserFace');
        const savedUserName = localStorage.getItem('registeredUserName');
        
        if (savedUserFace && savedUserName) {
          setRegisteredUserFace(JSON.parse(savedUserFace));
          setRegisteredUserName(savedUserName);
          console.log('Loaded registered user face from local storage');
        }
      } catch (error) {
        console.error('Error loading registered face:', error);
      }
    };
    
    loadUserFace();
  }, []);
  
  // Update faceDescriptors when a user registers their face
  useEffect(() => {
    if (registeredUserFace && registeredUserName && faceDescriptors.length > 0) {
      // Create a new array to avoid mutation
      const updatedDescriptors = [...faceDescriptors];
      
      // Check if user is already in the descriptors
      const userIndex = updatedDescriptors.findIndex(face => face.name === registeredUserName);
      
      const userFaceData = {
        id: userIndex >= 0 ? updatedDescriptors[userIndex].id : faceDescriptors.length + 1,
        name: registeredUserName,
        role: 'Registered User',
        confidence: 0.98,
        authorized: true,
        lastSeen: 'Just now',
        imageUrl: registeredUserFace.imageUrl,
        descriptor: registeredUserFace.descriptor
      };
      
      if (userIndex >= 0) {
        // Update existing user
        updatedDescriptors[userIndex] = userFaceData;
      } else {
        // Add new user
        updatedDescriptors.push(userFaceData);
      }
      
      setFaceDescriptors(updatedDescriptors);
      console.log('Updated face descriptors with registered user face');
    }
  }, [registeredUserFace, registeredUserName, faceDescriptors]);

  const createImageElement = (url: string): Promise<HTMLImageElement | null> => {
    return new Promise((resolve) => {
      // Fix: Don't use new Image() constructor, create the element directly
      const img = document.createElement('img') as HTMLImageElement;
      img.crossOrigin = 'anonymous';
      img.onload = () => resolve(img);
      img.onerror = () => {
        console.error(`Failed to load image: ${url}`);
        resolve(null);
      };
      img.src = url;
    });
  };
  

  
  // Handle sample image selection
  const handleSampleImage = (sampleNumber: number) => {
    const selectedFace = registeredFaces.find(face => face.id === sampleNumber);
    if (selectedFace) {
      setUploadedImage(selectedFace.imageUrl);
      processImage();
    } else {
      setUploadedImage(`/samples/face-${sampleNumber}.jpg`);
    }
    setDemoStage('upload');
  }
  
  // Create a completely new approach for the startCamera function
  const startCamera = async () => {
    try {
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        setCameraError('Your browser does not support camera access');
        return;
      }

      // First check if camera permissions are already granted
      try {
        const permissionStatus = await navigator.permissions.query({
          name: 'camera' as PermissionName
        });
        
        if (permissionStatus.state === 'denied') {
          setCameraError('Camera access is blocked. Please allow camera access in your browser settings.');
          return;
        }
      } catch (permErr) {
        console.warn('Unable to check camera permissions:', permErr);
      }

      // Show a loading indicator
      const loadingIndicator = document.createElement('div');
      loadingIndicator.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
      loadingIndicator.innerHTML = `
        <div class="bg-white p-6 rounded-lg shadow-lg text-center">
          <div class="animate-spin rounded-full h-10 w-10 border-b-2 border-blue-600 mx-auto mb-4"></div>
          <p class="text-lg font-medium">Activating Camera...</p>
          <p class="text-sm text-gray-500 mt-2">You may need to grant camera permission</p>
          <p class="text-xs text-blue-500 mt-3" id="camera-status">Initializing camera...</p>
        </div>
      `;
      document.body.appendChild(loadingIndicator);
      
      // Status update function
      const updateStatus = (message: string) => {
        const statusEl = document.getElementById('camera-status');
        if (statusEl) statusEl.textContent = message;
      };

      // Set state to indicate camera activation started
      setUsingCamera(true);
      setCameraError(null);

      try {
        // IMPORTANT CHANGE: Create a temporary container and video for camera access
        // instead of relying on React to render it first
        
        // Create a temporary off-screen camera container
        const tempContainer = document.createElement('div');
        tempContainer.id = 'temp-camera-container';
        tempContainer.style.position = 'fixed';
        tempContainer.style.top = '0';
        tempContainer.style.left = '0';
        tempContainer.style.opacity = '0';
        tempContainer.style.pointerEvents = 'none';
        document.body.appendChild(tempContainer);
        
        updateStatus('Created temporary camera container');
        
        // Create the video element
        const tempVideo = document.createElement('video');
        tempVideo.autoplay = true;
        tempVideo.playsInline = true;
        tempVideo.muted = true;
        tempVideo.style.width = '320px';
        tempVideo.style.height = '240px';
        tempContainer.appendChild(tempVideo);
        
        updateStatus('Requesting camera access...');
        
        // Request camera access
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { 
            facingMode: 'user',
            width: { ideal: 640 },
            height: { ideal: 480 }
          }
        });
        
        updateStatus('Camera access granted!');
        
        // Attach stream to temp video
        tempVideo.srcObject = stream;
        
        // Wait for metadata to be loaded to ensure video is ready
        await new Promise<void>((resolve) => {
          tempVideo.onloadedmetadata = () => {
            console.log('Video metadata loaded');
            resolve();
          };
          
          // Add a timeout just in case
    setTimeout(() => {
            if (!tempVideo.videoWidth) {
              console.log('Video metadata load timeout - continuing anyway');
              resolve();
            }
          }, 2000);
        });
        
        console.log('â Camera activated successfully with dimensions:', 
          tempVideo.videoWidth, 'x', tempVideo.videoHeight);
        
        // Stream is active, now we need to connect it to the React UI
        // Create a global function to allow retry from UI
        window.connectCameraStream = () => {
          // Find all potential camera containers
          console.log('Searching for camera container in DOM...');
          
          const cameraContainerSelectors = [
            '#camera-container',
            '[id="camera-container"]',
            '.relative.w-full.max-w-md.mx-auto.border.rounded-lg.overflow-hidden',
            // Add more potential selectors here
          ];
          
          // Log all top-level elements to help debug
          console.log('Current DOM structure:', 
            Array.from(document.body.children)
              .map(el => `${el.tagName}${el.id ? '#'+el.id : ''}.${el.className.split(' ').join('.')}`)
              .join(', ')
          );
          
          // Try each selector
          let reactContainer = null;
          for (const selector of cameraContainerSelectors) {
            const container = document.querySelector(selector);
            if (container) {
              console.log(`Found camera container using selector: ${selector}`);
              reactContainer = container;
              break;
            }
          }
          
          if (!reactContainer) {
            console.log('Camera container not found. Creating a fallback container.');
            
            // Check if we already have a fallback
            let fallbackContainer = document.getElementById('fallback-camera-container');
            
            if (!fallbackContainer) {
              // Create a fallback container if the React one is not found
              fallbackContainer = document.createElement('div');
              fallbackContainer.id = 'fallback-camera-container';
              fallbackContainer.className = 'relative w-full max-w-md mx-auto border border-red-500 rounded-lg overflow-hidden';
              fallbackContainer.style.height = '320px';
              
              // Add a clear message about the fallback
              const message = document.createElement('div');
              message.className = 'absolute bottom-0 left-0 right-0 bg-red-500 text-white p-2 text-center text-sm';
              message.textContent = 'Using fallback camera view - React container not found';
              fallbackContainer.appendChild(message);
              
              // Find a good place to insert it
              const possibleContainers = [
                document.querySelector('.w-full.max-w-md'),
                document.querySelector('.max-w-5xl.mx-auto.p-4'),
                document.body
              ];
              
              let insertTarget = null;
              for (const container of possibleContainers) {
                if (container) {
                  insertTarget = container;
                  break;
                }
              }
              
              if (insertTarget === document.body) {
                // If we're appending to body, make it a modal
                const modal = document.createElement('div');
                modal.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
                
                const modalContent = document.createElement('div');
                modalContent.className = 'bg-white p-6 rounded-lg shadow-lg max-w-md w-full';
                modalContent.innerHTML = `
                  <h3 class="text-xl font-bold mb-3">Camera Feed</h3>
                  <p class="mb-3 text-red-600">React UI not found. Using fallback camera view.</p>
                `;
                
                modal.appendChild(modalContent);
                modalContent.appendChild(fallbackContainer);
                document.body.appendChild(modal);
              } else if (insertTarget) {
                // Insert before the first child
                insertTarget.insertBefore(fallbackContainer, insertTarget.firstChild);
      } else {
                // Fallback if no container is found
                document.body.appendChild(fallbackContainer);
              }
            }
            
            reactContainer = fallbackContainer;
          }
          
          // Now create/find a video element in the container
          let reactVideo = reactContainer.querySelector('video') as HTMLVideoElement;
          
          if (!reactVideo) {
            console.log('Video element not found in container, creating one');
            reactVideo = document.createElement('video');
            reactVideo.autoplay = true;
            reactVideo.playsInline = true;
            reactVideo.muted = true;
            reactVideo.className = 'w-full h-auto';
            
            // Make sure the video fills the container
            reactVideo.style.width = '100%';
            reactVideo.style.height = '100%';
            reactVideo.style.objectFit = 'cover';
            
            reactContainer.appendChild(reactVideo);
          }
          
          // Connect the stream
          if (tempVideo && tempVideo.srcObject) {
            console.log('Connecting active stream to React video element');
            reactVideo.srcObject = tempVideo.srcObject;
            
            // Also update the ref if available
            if (videoRef.current) {
              videoRef.current.srcObject = tempVideo.srcObject;
            }
            
            // Add flash effect
            reactContainer.classList.add('camera-active-flash');
            setTimeout(() => {
              reactContainer.classList.remove('camera-active-flash');
            }, 2000);
            
            // Show a success toast
            const toast = document.createElement('div');
            toast.className = 'fixed bottom-4 right-4 bg-green-600 text-white px-4 py-2 rounded-lg shadow-lg transition-opacity duration-500';
            toast.innerHTML = `
              <div class="flex items-center">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="mr-2"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                <span>Camera connected successfully</span>
              </div>
            `;
            document.body.appendChild(toast);
            
            setTimeout(() => {
              toast.style.opacity = '0';
              setTimeout(() => {
                if (document.body.contains(toast)) {
                  document.body.removeChild(toast);
                }
              }, 500);
            }, 3000);
            
            return true;
          } else {
            console.error('No active stream to connect');
            return false;
          }
        };
        
        // Try to transfer the stream to React component multiple times with increasing delays
        const transferAttempts = [100, 300, 500, 1000, 2000]; // milliseconds
        
        for (const delay of transferAttempts) {
          await new Promise(resolve => setTimeout(resolve, delay));
          console.log(`Attempting to transfer stream after ${delay}ms delay...`);
          
          if (window.connectCameraStream && window.connectCameraStream()) {
            console.log(`Successfully transferred stream after ${delay}ms`);
            break;
          }
        }
        
        // Remove loading indicator
        if (document.body.contains(loadingIndicator)) {
          document.body.removeChild(loadingIndicator);
        }
        
        // If after all attempts we still don't have the video connected to React
        // Show a message with a manual connect button
        setTimeout(() => {
          const videoElement = document.querySelector('#camera-container video, #fallback-camera-container video');
          
          if (!videoElement || !(videoElement as HTMLVideoElement).srcObject) {
            console.log('Still no video in UI after all attempts, showing manual connect button');
            
            const manualConnectNotice = document.createElement('div');
            manualConnectNotice.className = 'fixed bottom-4 left-1/2 transform -translate-x-1/2 bg-yellow-50 border border-yellow-400 text-yellow-800 px-4 py-3 rounded-lg shadow-lg';
            manualConnectNotice.innerHTML = `
              <div class="flex items-center">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="mr-2"><path d="M10.29 3.86L1.82 18a2 2 0 0 0 1.71 3h16.94a2 2 0 0 0 1.71-3L13.71 3.86a2 2 0 0 0-3.42 0z"></path><line x1="12" y1="9" x2="12" y2="13"></line><line x1="12" y1="17" x2="12.01" y2="17"></line></svg>
                <div>
                  <p class="font-bold">Camera is active but not showing in UI</p>
                  <p class="text-sm">Click the button below to manually connect it</p>
                </div>
              </div>
              <button id="manual-connect-btn" class="mt-2 w-full bg-yellow-600 hover:bg-yellow-700 text-white font-bold py-2 px-4 rounded">
                Connect Camera Feed
              </button>
            `;
            document.body.appendChild(manualConnectNotice);
            
            document.getElementById('manual-connect-btn')?.addEventListener('click', () => {
              if (window.connectCameraStream && window.connectCameraStream()) {
                if (document.body.contains(manualConnectNotice)) {
                  document.body.removeChild(manualConnectNotice);
                }
              }
            });
            
            // Auto-remove after 20 seconds
            setTimeout(() => {
              if (document.body.contains(manualConnectNotice)) {
                document.body.removeChild(manualConnectNotice);
              }
            }, 20000);
          }
        }, 3000);
        
      } catch (error) {
        console.error('Error accessing webcam:', error);
        updateStatus('Error: ' + (error as Error).message);
        
        // Wait a moment then remove loading indicator and show error
        setTimeout(() => {
          if (document.body.contains(loadingIndicator)) {
            document.body.removeChild(loadingIndicator);
          }
          setCameraError('Could not access camera. Please ensure you have granted camera permissions and try again.');
          setUsingCamera(false);
        }, 2000);
      }
    } catch (error) {
      console.error('Error starting camera:', error);
      setCameraError('Could not access camera. Please ensure you have granted camera permissions.');
      setUsingCamera(false);
    }
  };

  // Update the stopCamera function to handle the temporary container
  const stopCamera = () => {
    // First check for the temp container
    const tempContainer = document.getElementById('temp-camera-container');
    if (tempContainer) {
      const tempVideo = tempContainer.querySelector('video');
      if (tempVideo && tempVideo.srcObject) {
        const stream = tempVideo.srcObject as MediaStream;
        const tracks = stream.getTracks();
        tracks.forEach(track => track.stop());
        tempVideo.srcObject = null;
      }
      
      // Remove the temp container
      document.body.removeChild(tempContainer);
    }
    
    // Then handle the React video if it exists
    if (videoRef.current && videoRef.current.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      const tracks = stream.getTracks();
      tracks.forEach(track => track.stop());
      videoRef.current.srcObject = null;
    }
    
    // Update state
    setUsingCamera(false);
  };

  // Capture frame from webcam
  const captureFrame = async () => {
    if (!videoRef.current || !canvasRef.current) return;
    
    const video = videoRef.current;
    const canvas = canvasRef.current;
    const context = canvas.getContext('2d');
    
    if (!context) return;
    
    // Set canvas dimensions to match video
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    
    // Draw video frame on canvas
    context.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Convert canvas to data URL
    const imageDataUrl = canvas.toDataURL('image/jpeg');
    setUploadedImage(imageDataUrl);
    
    // Stop the camera after capturing
    stopCamera();
  };
  
  // Process the captured frame to extract face descriptor
  const extractUserFaceDescriptor = async (imageDataUrl: string) => {
    if (!isModelLoaded) return null;
    
    try {
      const img = await createImageElement(imageDataUrl);
      if (!img) return null;
      
      // @ts-ignore - face-api is loaded from CDN
      const detection = await window.faceapi.detectSingleFace(img, new window.faceapi.TinyFaceDetectorOptions({
        inputSize: 416,
        scoreThreshold: 0.5
      }))
        // @ts-ignore - face-api is loaded from CDN
        .withFaceLandmarks()
        // @ts-ignore - face-api is loaded from CDN
        .withFaceDescriptor();
      
      if (detection) {
        setUserFaceDescriptor(detection.descriptor);
        return detection.descriptor;
      }
      
      return null;
    } catch (error) {
      console.error('Error extracting face descriptor:', error);
      return null;
    }
  };
  
  // Authenticate user against registered faces
  const authenticateUser = async (userDescriptor: any) => {
    if (!userDescriptor || faceDescriptors.length === 0) return null;
    
    let bestMatch = null;
    let highestConfidence = 0;
    
    for (const face of faceDescriptors) {
      if (face.descriptor) {
        // @ts-ignore - face-api is loaded from CDN
        const distance = window.faceapi.euclideanDistance(userDescriptor, face.descriptor);
        // Convert distance to confidence (1 - distance, normalized)
        const confidence = Math.max(0, 1 - distance);
        
        if (confidence > highestConfidence && confidence >= confidenceThreshold) {
          highestConfidence = confidence;
          bestMatch = {
            ...face,
            confidence: confidence
          };
        }
      }
    }
    
    // Only show authorized personnel if that option is selected
    if (bestMatch && (!isAuthorizedOnly || bestMatch.authorized)) {
      setAuthenticatedUser(bestMatch);
      return bestMatch;
    }
    
    return null;
  };
  
  // Update the camera interface to show face detection overlay
  const captureAndAuthenticate = async () => {
    if (!videoRef.current || !canvasRef.current) return;
    
    // First show a preview of the captured image
    const video = videoRef.current;
    const canvas = canvasRef.current;
    const context = canvas.getContext('2d');
    
    if (!context) {
      throw new Error('Failed to get canvas context');
    }
    
    // Set canvas dimensions to match video
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    
    // Draw video frame on canvas
    context.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Convert canvas to data URL
    const imageDataUrl = canvas.toDataURL('image/jpeg');
    
    // Show preview with confirmation
    const previewOverlay = document.createElement('div');
    previewOverlay.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-70 z-50';
    previewOverlay.innerHTML = `
      <div class="bg-white p-6 rounded-lg shadow-lg max-w-md w-full">
        <h3 class="text-xl font-bold mb-3">Confirm Your Face Image</h3>
        <div class="border rounded-lg overflow-hidden mb-4">
          <img src="${imageDataUrl}" alt="Your face" class="w-full">
        </div>
        <p class="mb-4 text-gray-600">This is the image that will be used for authentication. Is your face clearly visible?</p>
        <div class="flex justify-between">
          <button id="retake" class="px-4 py-2 bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition-colors">Retake</button>
          <button id="confirmCapture" class="px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition-colors">Confirm & Authenticate</button>
        </div>
      </div>
    `;
    document.body.appendChild(previewOverlay);
    
    // Set up event listeners for the buttons
    document.getElementById('retake')?.addEventListener('click', () => {
      document.body.removeChild(previewOverlay);
    });
    
    document.getElementById('confirmCapture')?.addEventListener('click', () => {
      document.body.removeChild(previewOverlay);
      
      // Now proceed with authentication
      setUploadedImage(imageDataUrl);
      
      // Show visual feedback that we're processing
      const processingMessage = document.createElement('div');
      processingMessage.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
      processingMessage.innerHTML = `
        <div class="bg-white p-6 rounded-lg shadow-lg">
          <div class="animate-spin rounded-full h-10 w-10 border-b-2 border-blue-600 mx-auto mb-4"></div>
          <p class="text-lg font-medium">Authenticating your face...</p>
        </div>
      `;
      document.body.appendChild(processingMessage);
      
      try {
        // Stop the camera after capturing
        stopCamera();
        
        // Remove processing message
        setTimeout(() => {
          if (document.body.contains(processingMessage)) {
            document.body.removeChild(processingMessage);
          }
          
          // Process the image
          processImage();
          
          // Log to console for the user
          console.log("â Face image captured and saved:", {
            imageSize: imageDataUrl.length,
            timestamp: new Date().toISOString()
          });
        }, 1000);
        
      } catch (error) {
        console.error('Error capturing and authenticating:', error);
        
        // Remove processing message
        if (document.body.contains(processingMessage)) {
          document.body.removeChild(processingMessage);
        }
        
        // Show error message
        alert('Error capturing image: ' + (error as Error).message);
        stopCamera();
      }
    });
  };

  // Add a face detection overlay to the camera view for better positioning
  const CameraViewWithDetection = () => {
    useEffect(() => {
      // Skip if no video element
      if (!videoRef.current) return;
      
      // Create overlay for face positioning
      const videoContainer = videoRef.current.parentElement;
      if (!videoContainer) return;
      
      // Create overlay element if it doesn't exist
      let overlay = videoContainer.querySelector('.face-detection-overlay');
      if (!overlay) {
        overlay = document.createElement('div');
        overlay.className = 'face-detection-overlay absolute inset-0 pointer-events-none';
        videoContainer.appendChild(overlay);
      }
      
      // Add the face guide
      overlay.innerHTML = `
        <div class="absolute inset-0 flex items-center justify-center">
          <div class="w-64 h-64 border-4 border-dashed border-blue-400 rounded-full opacity-70"></div>
        </div>
        <div class="absolute bottom-3 left-0 right-0 text-center text-white text-sm bg-black bg-opacity-50 py-1">
          Position your face inside the circle
        </div>
      `;
      
      // Periodically check for face in the video
      const checkInterval = setInterval(() => {
        // Only proceed if we have video feed
        if (!videoRef.current || !videoRef.current.videoWidth) return;
        
        // Use a temporary canvas to check for face
        const tempCanvas = document.createElement('canvas');
        const context = tempCanvas.getContext('2d', { willReadFrequently: true });
        if (!context || !videoRef.current) return;
        
        tempCanvas.width = videoRef.current.videoWidth;
        tempCanvas.height = videoRef.current.videoHeight;
        
        // Draw current video frame
        context.drawImage(videoRef.current, 0, 0, tempCanvas.width, tempCanvas.height);
        
        // Simple face detection - check for skin tones in center area
        const centerX = tempCanvas.width / 2;
        const centerY = tempCanvas.height / 2;
        const radius = Math.min(tempCanvas.width, tempCanvas.height) * 0.2;
        
        // Sample pixels in the center area
        const imageData = context.getImageData(centerX - radius, centerY - radius, radius * 2, radius * 2);
        const data = imageData.data;
        
        // Count potential skin tone pixels
        let skinTonePixels = 0;
        const totalPixels = data.length / 4;
        
        for (let i = 0; i < data.length; i += 4) {
          const r = data[i];
          const g = data[i + 1];
          const b = data[i + 2];
          
          // Basic skin tone detection
          if (r > 60 && g > 40 && b > 20 && 
              r > g && g > b && 
              r - g > 15 && 
              Math.abs(r - g) < 100) {
            skinTonePixels++;
          }
        }
        
        // Calculate percentage of skin tone pixels
        const skinTonePercentage = (skinTonePixels / totalPixels) * 100;
        
        // Update the overlay to show face detection status
        if (skinTonePercentage > 30) {
          overlay.innerHTML = `
            <div class="absolute inset-0 flex items-center justify-center">
              <div class="w-64 h-64 border-4 border-green-500 rounded-full opacity-70"></div>
            </div>
            <div class="absolute bottom-3 left-0 right-0 text-center text-white bg-green-600 bg-opacity-90 py-1 font-medium">
              â Face Detected - Ready to capture
            </div>
          `;
          console.log("â Face detected in camera view!");
        } else {
          overlay.innerHTML = `
            <div class="absolute inset-0 flex items-center justify-center">
              <div class="w-64 h-64 border-4 border-dashed border-yellow-400 rounded-full opacity-70"></div>
            </div>
            <div class="absolute bottom-3 left-0 right-0 text-center text-white bg-black bg-opacity-50 py-1">
              Position your face inside the circle
            </div>
          `;
        }
      }, 500);
      
      // Clean up on unmount
      return () => {
        clearInterval(checkInterval);
        if (overlay && overlay.parentNode) {
          overlay.parentNode.removeChild(overlay);
        }
      };
    }, [usingCamera]);
    
    return null;
  };

  // Update the registerUserFace function to add confirmation dialog
  const registerUserFace = async () => {
    if (!videoRef.current || !canvasRef.current) return;
    
    // First capture the image for preview
    const video = videoRef.current;
    const canvas = canvasRef.current;
    const context = canvas.getContext('2d');
    
    if (!context) {
      throw new Error('Failed to get canvas context');
    }
    
    // Set canvas dimensions to match video
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    
    // Draw video frame on canvas
    context.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Convert canvas to data URL
    const imageDataUrl = canvas.toDataURL('image/jpeg');
    
    // Show preview with confirmation
    const previewOverlay = document.createElement('div');
    previewOverlay.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-70 z-50';
    previewOverlay.innerHTML = `
      <div class="bg-white p-6 rounded-lg shadow-lg max-w-md w-full">
        <h3 class="text-xl font-bold mb-3">Confirm Your Face Registration</h3>
        <div class="border rounded-lg overflow-hidden mb-4">
          <img src="${imageDataUrl}" alt="Your face" class="w-full">
        </div>
        <p class="mb-4 text-gray-600">This image will be used to identify you in the future. Is your face clearly visible?</p>
        <div class="flex justify-between">
          <button id="retakeRegistration" class="px-4 py-2 bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition-colors">Retake</button>
          <button id="confirmRegistration" class="px-4 py-2 bg-green-600 text-white rounded-md hover:bg-green-700 transition-colors">Confirm & Register</button>
        </div>
      </div>
    `;
    document.body.appendChild(previewOverlay);
    
    // Set up event listeners for the buttons
    document.getElementById('retakeRegistration')?.addEventListener('click', () => {
      document.body.removeChild(previewOverlay);
    });
    
    document.getElementById('confirmRegistration')?.addEventListener('click', () => {
      document.body.removeChild(previewOverlay);
      
      // Show visual feedback that we're processing
      const processingMessage = document.createElement('div');
      processingMessage.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
      processingMessage.innerHTML = `
        <div class="bg-white p-6 rounded-lg shadow-lg">
          <div class="animate-spin rounded-full h-10 w-10 border-b-2 border-blue-600 mx-auto mb-4"></div>
          <p class="text-lg font-medium">Processing your face...</p>
        </div>
      `;
      document.body.appendChild(processingMessage);
      
      // Proceed with the original registration logic
      try {
        // Log to console for the user to see
        console.log("â Face image captured for registration:", {
          name: registeredUserName,
          imageSize: imageDataUrl.length,
          timestamp: new Date().toISOString()
        });
        
        // Continue with face detection and registration
        processRegistration(imageDataUrl, processingMessage);
        
      } catch (error) {
        console.error('Error in registration preview:', error);
        
        // Remove processing message
        document.body.removeChild(processingMessage);
        
        // Show error message
        alert('Error capturing image: ' + (error as Error).message);
      }
    });
  };

  // Create a separate function to process registration
  const processRegistration = async (imageDataUrl: string, processingMessage: HTMLElement) => {
    try {
      // Create an image element for face detection
      const img = await createImageElement(imageDataUrl);
      if (!img) {
        throw new Error('Failed to create image from camera capture');
      }
      
      console.log('Attempting to detect face for registration...');
      
      // Try to detect face using face-api.js if available
      let faceData;
      
      if (window.faceapi) {
        try {
          // @ts-ignore - face-api is loaded from CDN
          const detection = await window.faceapi.detectSingleFace(img, new window.faceapi.TinyFaceDetectorOptions({
            inputSize: 416,
            scoreThreshold: 0.5
          }))
            // @ts-ignore - face-api is loaded from CDN
            .withFaceLandmarks()
            // @ts-ignore - face-api is loaded from CDN
            .withFaceDescriptor();
          
          if (detection) {
            console.log('Face detected for registration with face-api.js');
            faceData = {
              descriptor: detection.descriptor,
              imageUrl: imageDataUrl
            };
          } else {
            throw new Error('No face detected in the image');
          }
        } catch (faceApiError) {
          console.error('Error detecting face with face-api.js:', faceApiError);
          
          // Create a mock descriptor as fallback
          const mockDescriptor = Array(128).fill(0).map((_, i) => {
            const nameSeed = registeredUserName.charCodeAt(i % registeredUserName.length) / 255;
            const imgSeed = imageDataUrl.charCodeAt(i % 100) / 255;
            return (Math.sin(nameSeed * imgSeed * (i + 1)) * 0.5 + 0.5);
          });
          
          faceData = {
            descriptor: mockDescriptor,
            imageUrl: imageDataUrl
          };
        }
      } else {
        // If face-api is not available, create a mock descriptor
        console.log('Face-api not available, creating mock descriptor');
        const mockDescriptor = Array(128).fill(0).map((_, i) => {
          const nameSeed = registeredUserName.charCodeAt(i % registeredUserName.length) / 255;
          const imgSeed = imageDataUrl.charCodeAt(i % 100) / 255;
          return (Math.sin(nameSeed * imgSeed * (i + 1)) * 0.5 + 0.5);
        });
        
        faceData = {
          descriptor: mockDescriptor,
          imageUrl: imageDataUrl
        };
      }
      
      // Save to state and localStorage
      setRegisteredUserFace(faceData);
      localStorage.setItem('registeredUserFace', JSON.stringify(faceData));
      localStorage.setItem('registeredUserName', registeredUserName);
      
      // Update face descriptors array
      const userFaceData = {
        id: faceDescriptors.length + 1,
        name: registeredUserName,
        role: 'Registered User',
        confidence: 0.98,
        authorized: true,
        lastSeen: 'Just now',
        imageUrl: imageDataUrl,
        descriptor: faceData.descriptor
      };
      
      // Add to face descriptors
      setFaceDescriptors([...faceDescriptors, userFaceData]);
      console.log('Added user face to face descriptors');
      
      // Set isFaceDB to true to indicate we have face descriptors
      setIsFaceDB(true);
      
      // Set isModelLoaded to true even if we're using a mock descriptor
      // This will prevent falling back to simulation mode
      setIsModelLoaded(true);
      
      // Stop camera and show success with more visible feedback
      stopCamera();
      setRegistrationMode(false);
      
      // Remove processing message
      document.body.removeChild(processingMessage);
      
      // Show success with more visible feedback
      const successMessage = document.createElement('div');
      successMessage.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
      successMessage.innerHTML = `
        <div class="bg-white p-6 rounded-lg shadow-lg">
          <div class="text-green-500 flex justify-center mb-3">
            <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path>
              <polyline points="22 4 12 14.01 9 11.01"></polyline>
            </svg>
          </div>
          <p class="text-xl font-medium text-center">Face Registered Successfully!</p>
          <p class="text-gray-600 text-center mt-2">Your face has been saved with a unique identifier. You can now authenticate with your face.</p>
          <div class="mt-4 text-center">
            <button class="px-4 py-2 bg-blue-600 text-white rounded-md" id="successDismiss">Continue</button>
          </div>
        </div>
      `;
      document.body.appendChild(successMessage);
      
      // Add event listener to dismiss button
      document.getElementById('successDismiss')?.addEventListener('click', () => {
        document.body.removeChild(successMessage);
      });
      
      // Auto-dismiss after 5 seconds
      setTimeout(() => {
        if (document.body.contains(successMessage)) {
          document.body.removeChild(successMessage);
        }
      }, 5000);
      
      setShowRegistrationSuccess(true);
      
      // Hide success message after 3 seconds
      setTimeout(() => {
        setShowRegistrationSuccess(false);
      }, 3000);
      
    } catch (error) {
      console.error('Error registering face:', error);
      
      // Remove processing message
      document.body.removeChild(processingMessage);
      
      // Show error message
      alert('Error registering face: ' + (error as Error).message);
      stopCamera();
      setRegistrationMode(false);
    }
  };
  
  // Reset the demo
  const resetDemo = () => {
    setUploadedImage(null);
    setDemoStage('upload');
    setRecognitionResult(null);
    if (usingCamera) {
      stopCamera();
    }
  };
  
  // Handle completing the challenge
  const handleCompleteChallenge = () => {
    // Check if user has completed a full demo cycle to mark as complete
    if (!recognitionResult && demoStage !== 'result') {
      alert('Please complete the facial recognition demo before finishing the challenge.');
      return;
    }
    
    markChallengeAsCompleted('challenge-7');
    setIsCompleted(true);
    
    // Show confetti
    setShowConfetti(true);
    
    // Hide confetti after 5 seconds
    setTimeout(() => {
      setShowConfetti(false);
    }, 5000);
  };
  
  // Create sample faces if they don't exist
  const createSampleFaces = useCallback(() => {
    // Create a canvas element for generating placeholder faces
    const canvas = document.createElement('canvas');
    canvas.width = 400;
    canvas.height = 400;
    const ctx = canvas.getContext('2d');
    
    if (!ctx) return;

    // Create placeholder images for each registered face
    const colors = ['#f87171', '#60a5fa', '#a3e635', '#c084fc', '#fcd34d'];
    
    // First, create a placeholder for Ernesto Lee in case the image doesn't exist
    const ernesto = registeredFaces[0];
    
    // Draw Ernesto's placeholder image (will be used as fallback if the real image doesn't load)
    ctx.fillStyle = '#f87171'; // Red background for Ernesto
    ctx.fillRect(0, 0, 400, 400);
    
    // Add initials
    ctx.fillStyle = 'white';
    ctx.font = 'bold 120px Arial';
    ctx.textAlign = 'center';
    ctx.textBaseline = 'middle';
    
    const ernestoInitials = ernesto.name.split(' ').map(name => name[0]).join('');
    ctx.fillText(ernestoInitials, 200, 200);
    
    // Create a fallback image URL for Ernesto
    const ernestoFallbackUrl = canvas.toDataURL('image/jpeg');
    
    // Test if Ernesto's actual image exists and use fallback if it doesn't
    const ernestoImg = new Image();
    ernestoImg.onload = () => {
      console.log("Ernesto's image loaded successfully");
    };
    ernestoImg.onerror = () => {
      console.log("Ernesto's image failed to load, using placeholder");
      ernesto.imageUrl = ernestoFallbackUrl;
    };
    ernestoImg.src = ernesto.imageUrl;
    
    // Now create images for other users
    for (let i = 1; i < registeredFaces.length; i++) {
      const face = registeredFaces[i];
      if (!face.imageUrl || face.imageUrl.startsWith('/samples/')) {
        // Fill background with a distinct color
        ctx.fillStyle = colors[i % colors.length];
        ctx.fillRect(0, 0, 400, 400);
        
        // Add initials
        ctx.fillStyle = 'white';
        ctx.font = 'bold 120px Arial';
        ctx.textAlign = 'center';
        ctx.textBaseline = 'middle';
        
        const initials = face.name.split(' ').map(name => name[0]).join('');
        ctx.fillText(initials, 200, 200);
        
        // Save as data URL
        const dataUrl = canvas.toDataURL('image/jpeg');
        face.imageUrl = dataUrl;
      }
    }
    
    console.log('Sample face images created');
  }, []);
  
  // Call createSampleFaces on mount
  useEffect(() => {
    createSampleFaces();
  }, [createSampleFaces]);
  
  // Start registration process
  const startRegistration = () => {
    // Show instructions before starting
    const instructionsMessage = document.createElement('div');
    instructionsMessage.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
    instructionsMessage.innerHTML = `
      <div class="bg-white p-6 rounded-lg shadow-lg max-w-md">
        <h3 class="text-xl font-bold mb-3">Face Registration Instructions</h3>
        <ul class="list-disc pl-5 mb-4 space-y-2">
          <li>Position your face in the center of the camera</li>
          <li>Ensure good lighting on your face</li>
          <li>Remove glasses, masks or other face coverings</li>
          <li>Keep a neutral expression</li>
          <li>Hold still when taking the picture</li>
        </ul>
        <p class="text-sm text-gray-600 mb-4">This will allow the system to recognize you more accurately later.</p>
        <div class="flex justify-end space-x-3">
          <button class="px-3 py-1 bg-gray-200 rounded-md" id="cancelRegistrationBtn">Cancel</button>
          <button class="px-3 py-1 bg-blue-600 text-white rounded-md" id="proceedRegistrationBtn">Proceed</button>
        </div>
      </div>
    `;
    document.body.appendChild(instructionsMessage);
    
    // Add event listeners
    document.getElementById('cancelRegistrationBtn')?.addEventListener('click', () => {
      document.body.removeChild(instructionsMessage);
    });
    
    document.getElementById('proceedRegistrationBtn')?.addEventListener('click', () => {
      document.body.removeChild(instructionsMessage);
      setRegistrationMode(true);
      startCamera();
    });
  };
  
  // Add toggle between local detection and face-api.js
  const toggleDetectionMode = () => {
    setUseLocalDetection(!useLocalDetection);
  };

  // Add back the cancelRegistration function
  const cancelRegistration = () => {
    stopCamera();
    setRegistrationMode(false);
  };
  
  // Make sure processImage still exists in the code
  // Process image using local detection when face-api.js is not available
  const processImage = async () => {
    // Always start processing
    setDemoStage('processing');
    
    // Show a processing overlay for better user feedback
    const processingOverlay = document.createElement('div');
    processingOverlay.className = 'fixed top-0 left-0 w-full h-full flex items-center justify-center bg-black bg-opacity-50 z-50';
    processingOverlay.innerHTML = `
      <div class="bg-white p-6 rounded-lg shadow-lg">
        <div class="animate-spin rounded-full h-10 w-10 border-b-2 border-blue-600 mx-auto mb-4"></div>
        <p class="text-lg font-medium">Processing Face Recognition...</p>
      </div>
    `;
    document.body.appendChild(processingOverlay);
    
    if (!uploadedImage) {
      document.body.removeChild(processingOverlay);
      setRecognitionResult(null);
      setDemoStage('result');
      return;
    }
    
    // If models aren't loaded or face descriptors aren't available but local detection is enabled, use that
    if ((!isModelLoaded || faceDescriptors.length === 0) && useLocalDetection) {
      document.body.removeChild(processingOverlay);
      processImageLocally();
      return;
    } 
    
    try {
      // Create an image element from the uploaded image
      const img = await createImageElement(uploadedImage);
      
      if (!img) {
        throw new Error('Failed to load image');
      }
      
      console.log('Attempting real face detection...');
      
      // Check if we have a registered user face first - prioritize matching against registered user
      if (registeredUserFace && registeredUserName) {
        console.log('Checking against registered user face');
        
        // Use the face detection from face-api
        const faceDetectionResult = await detectFaceWithFaceAPI(img);
        
        if (faceDetectionResult) {
          const {detection, descriptor} = faceDetectionResult;
          console.log('Face detected with face-api.js');
          
          // Now find the best match against the registered user
          if (registeredUserFace.descriptor) {
            try {
              // @ts-ignore - face-api is loaded from CDN
              const distance = window.faceapi.euclideanDistance(descriptor, registeredUserFace.descriptor);
              const confidence = Math.max(0, 1 - distance);
              console.log('Distance to registered user:', distance, 'Confidence:', confidence);
              
              if (confidence >= confidenceThreshold) {
                const result = {
                  id: 999,
                  name: registeredUserName,
                  role: 'Registered User',
                  confidence: confidence,
                  authorized: true,
                  lastSeen: 'Just now',
                  imageUrl: registeredUserFace.imageUrl
                };
                
                document.body.removeChild(processingOverlay);
                setRecognitionResult(result);
                setDemoStage('result');
                return;
              }
            } catch (error) {
              console.error('Error comparing with registered user:', error);
            }
          }
        }
      }
      
      // If no match with registered user or from library, use face-api detection for other faces
      const faceDetectionResult = await detectFaceWithFaceAPI(img);
      
      if (faceDetectionResult) {
        const {detection, descriptor} = faceDetectionResult;
        console.log('Face detected with face-api.js');
        
        // Check against other faces
        if (faceDescriptors.length > 0) {
          let bestMatch = null;
          let highestConfidence = 0;
          
          for (const face of faceDescriptors) {
            if (face.descriptor) {
              try {
                // @ts-ignore - face-api is loaded from CDN
                const distance = window.faceapi.euclideanDistance(descriptor, face.descriptor);
                const confidence = Math.max(0, 1 - distance);
                
                if (confidence > highestConfidence && confidence >= confidenceThreshold) {
                  highestConfidence = confidence;
                  bestMatch = {
                    ...face,
                    confidence: confidence
                  };
                }
              } catch (error) {
                console.error('Error comparing with face:', face.name, error);
              }
            }
          }
          
          // Only show authorized personnel if that option is selected
          if (bestMatch && (!isAuthorizedOnly || bestMatch.authorized)) {
            document.body.removeChild(processingOverlay);
            setRecognitionResult(bestMatch);
            setDemoStage('result');
            return;
          }
        }
      }
      
      // If we reach here, we didn't find a match
      console.log('No match found with real face recognition');
      document.body.removeChild(processingOverlay);
      setRecognitionResult(null);
      setDemoStage('result');
      
    } catch (error) {
      console.error('Error in face recognition:', error);
      document.body.removeChild(processingOverlay);
      
      // Try local detection as a last resort if enabled
      if (useLocalDetection) {
        processImageLocally();
      } else {
        // As a very last resort, show Ernesto for his image
        if (uploadedImage && uploadedImage.includes('ErnestoLee')) {
          setRecognitionResult({...registeredFaces[0], confidence: 0.97});
          setDemoStage('result');
        } else {
          setRecognitionResult(null);
          setDemoStage('result');
        }
      }
    }
  };

  // Helper function to detect a face using face-api.js
  const detectFaceWithFaceAPI = async (img: HTMLImageElement) => {
    if (!window.faceapi) {
      console.error('face-api.js not loaded');
      return null;
    }
    
    try {
      // @ts-ignore - face-api is loaded from CDN
      const detection = await window.faceapi.detectSingleFace(img, new window.faceapi.TinyFaceDetectorOptions({
        inputSize: 416,
        scoreThreshold: 0.5
      }))
        // @ts-ignore - face-api is loaded from CDN
        .withFaceLandmarks()
        // @ts-ignore - face-api is loaded from CDN
        .withFaceDescriptor();
      
      if (detection) {
        return {
          detection: detection,
          descriptor: detection.descriptor
        };
      }
      
      return null;
    } catch (error) {
      console.error('Error detecting face with face-api.js:', error);
      return null;
    }
  };

  // Process image using local detection when face-api.js is not available
  const processImageLocally = async () => {
    setDemoStage('processing');
    
    if (!uploadedImage) {
      setRecognitionResult(null);
      setDemoStage('result');
      return;
    }
    
    try {
      // Check if the image has a face
      console.log('Attempting local face detection...');
      
      // If this is a registered user trying to authenticate
      if (registeredUserFace && registeredUserName) {
        // Show processing result
        setRecognitionResult({
          id: 999,
          name: registeredUserName,
          role: 'Registered User',
          confidence: 0.92,
          authorized: true,
          lastSeen: 'Just now',
          imageUrl: registeredUserFace.imageUrl
        });
        setDemoStage('result');
        return;
      }
      
      // Find the matching face based on the image URL
      const matchedFace = registeredFaces.find(face => uploadedImage && uploadedImage.includes(face.imageUrl));
      
      if (matchedFace) {
        setRecognitionResult({...matchedFace, confidence: matchedFace.confidence});
      } else if (uploadedImage && uploadedImage.includes('/samples/')) {
        // If it's one of the sample images but no direct match, use a random face
        const randomIndex = Math.floor(Math.random() * registeredFaces.length);
        setRecognitionResult(registeredFaces[randomIndex]);
      } else {
        // For other images, random selection if it meets confidence threshold
        setRecognitionResult(null);
      }
      
      setDemoStage('result');
    } catch (error) {
      console.error('Error in local image processing:', error);
      setRecognitionResult(null);
      setDemoStage('result');
    }
  };

  // Add privacy text component
  const PrivacyNotice = () => (
    <div className="mt-4 p-3 bg-blue-50 rounded-md border border-blue-200">
      <h4 className="font-medium flex items-center text-blue-800">
        <Shield className="h-4 w-4 mr-2" /> Privacy Information
      </h4>
      <p className="text-sm text-gray-600 mt-1">
        <strong>Your data stays on your device.</strong> All face processing is done locally in your browser. 
        Images are only stored in your browser's local storage and are not sent to any server or cloud service.
        You can clear this data at any time using the "Clear Registration" button.
      </p>
    </div>
  );

  // Add a technology explanation component
  const HowItWorksModal = () => {
    const [isOpen, setIsOpen] = useState(false);

    return (
      <>
        <button
          onClick={() => setIsOpen(true)}
          className="text-sm text-blue-600 hover:text-blue-800 underline flex items-center mt-2"
        >
          <InfoIcon className="h-4 w-4 mr-1" /> How Face Recognition Works
        </button>

        {isOpen && (
          <div className="fixed inset-0 bg-black bg-opacity-50 z-50 flex items-center justify-center p-4">
            <div className="bg-white rounded-lg shadow-xl max-w-2xl w-full max-h-[90vh] overflow-y-auto">
              <div className="p-6">
                <div className="flex justify-between items-start mb-4">
                  <h2 className="text-xl font-bold">How Face Recognition Technology Works</h2>
                  <button 
                    onClick={() => setIsOpen(false)}
                    className="text-gray-500 hover:text-gray-700"
                  >
                    <XCircle className="h-6 w-6" />
                  </button>
                </div>

                <div className="space-y-4">
                  <section>
                    <h3 className="font-medium text-lg mb-2">Face Detection</h3>
                    <p className="text-gray-700">
                      First, the system needs to find where faces are in an image. It uses computer vision 
                      algorithms to identify facial features by analyzing patterns of dark and light in the image.
                    </p>
                  </section>

                  <section>
                    <h3 className="font-medium text-lg mb-2">Face Analysis</h3>
                    <p className="text-gray-700">
                      Once a face is detected, the system identifies key facial landmarks â like the position
                      of your eyes, nose, and mouth. It measures distances between these features and analyzes
                      their proportions.
                    </p>
                  </section>

                  <section>
                    <h3 className="font-medium text-lg mb-2">Creating a Face Descriptor</h3>
                    <p className="text-gray-700">
                      The measurements are converted into a mathematical representation called a "face descriptor"
                      â a unique numerical code (like a facial fingerprint) that represents your face. This is typically 
                      a vector with 128 numbers that captures the distinctive characteristics of your face.
                    </p>
                  </section>

                  <section>
                    <h3 className="font-medium text-lg mb-2">Face Matching</h3>
                    <p className="text-gray-700">
                      When you try to authenticate, a new face descriptor is created from the current 
                      image and compared with the stored descriptor. If they're similar enough (above the 
                      confidence threshold), it's considered a match.
                    </p>
                  </section>

                  <section>
                    <h3 className="font-medium text-lg mb-2">Local Processing in This Demo</h3>
                    <p className="text-gray-700">
                      In this demo, all processing happens locally in your browser using JavaScript. The face 
                      descriptors are stored only in your browser's local storage and are not sent to any server.
                      We use face-api.js, an open-source library that runs TensorFlow.js models directly in your browser.
                    </p>
                  </section>

                  <div className="bg-yellow-50 p-4 rounded-md mt-4">
                    <h3 className="font-medium text-yellow-800 mb-2">Privacy Notice</h3>
                    <p className="text-sm">
                      Your facial data never leaves your device. It's stored only in your browser's local storage
                      and is automatically cleared when you close your browser or manually clear it using the 
                      "Clear Registration" button. No images or facial data are ever sent to any server.
                    </p>
                  </div>
                </div>

                <div className="mt-6 text-right">
                  <button
                    onClick={() => setIsOpen(false)}
                    className="px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700"
                  >
                    Close
                  </button>
                </div>
              </div>
            </div>
          </div>
        )}
      </>
    );
  };

  // Add CSS classes for camera activation animation
  useEffect(() => {
    // Add CSS for camera active animation
    const style = document.createElement('style');
    style.textContent = `
      .camera-active-flash {
        animation: pulse-border 2s ease-out;
        position: relative;
      }
      
      @keyframes pulse-border {
        0% { box-shadow: 0 0 0 0 rgba(16, 185, 129, 0.7); }
        70% { box-shadow: 0 0 0 10px rgba(16, 185, 129, 0); }
        100% { box-shadow: 0 0 0 0 rgba(16, 185, 129, 0); }
      }
    `;
    document.head.appendChild(style);
    
    return () => {
      // Clean up
      document.head.removeChild(style);
    };
  }, []);

  // Add state to track navigation
  const [showImprovedVersion, setShowImprovedVersion] = useState<boolean>(false);
  
  return (
    <div className="max-w-5xl mx-auto p-4">
      <ChallengeHeader
        title="Challenge # 5: AI Facial Recognition How Machines Recognize Faces"
        icon={<Scan className="h-6 w-6 text-blue-600" />}
        challengeId="challenge-7"
        isCompleted={isCompleted}
        setIsCompleted={setIsCompleted}
        showConfetti={showConfetti}
        setShowConfetti={setShowConfetti}
        onCompleteChallenge={handleCompleteChallenge}
        isHPChallenge={true}
      />
      
      {/* Add How AI Works For You section */}
      <div className="mb-6 p-4 bg-blue-50 border border-blue-200 rounded-lg">
        <h3 className="font-medium text-blue-800 mb-2">How AI Works for You</h3>
        <p className="text-sm text-gray-600">
          AI-powered facial recognition is transforming security and access control, making identity verification fast, seamless, and smart. In this challenge, you'll select sample images and watch AI in action as it determines whether they are authorized users in a simulated business setting. Using deep learning, AI maps key facial featuresâlike eye distance and jawline shapeâto create a unique digital "faceprint" for verification. See firsthand how AI enhances security, speeds up check-ins, and ensures only the right people get access!
        </p>
      </div>
      
      {/* Challenge Steps Quick View */}
      <div className="mb-6 p-4 bg-white border border-gray-200 rounded-lg">
        <h3 className="font-medium text-gray-800 mb-2">Challenge Steps Quick View:</h3>
        <p className="text-sm text-gray-600 mb-2">There are 3 steps required to complete the Challenge.</p>
        <ul className="list-disc pl-6 text-sm text-gray-600">
          <li>Step 1: Read through Understanding Facial Recognition</li>
          <li>Step 2: Select from Sample Images to have AI perform facial recognition.</li>
          <li>Step 3: Challenge Completed!</li>
        </ul>
        <p className="text-sm text-gray-600 mt-2">Please review and follow each detailed step below.</p>
      </div>
      
      <div className="bg-white shadow-md rounded-lg p-6">
        <h2 className="text-xl font-bold mb-4">Take the Challenge!</h2>
        <h3 className="font-medium text-gray-800 mb-3">Step 1: Read through Understanding Facial Recognition to learn how AI performs this incredible task, and how business are applying this technology today.</h3>
        
        {/* Educational Content - Always visible */}
        <div className="mb-8 p-4 bg-gray-50 rounded-lg">
          <h3 className="text-lg font-medium mb-4">Understanding Facial Recognition</h3>
          <p className="text-gray-600 mb-4">Learn how facial recognition works and its applications in modern technology.</p>
          
          <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6">
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">What It Is</h4>
              <p className="text-sm text-gray-600">Facial recognition identifies or verifies a person using their unique facial features, creating a digital "faceprint" for comparison.</p>
            </div>
            
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">How It Works</h4>
              <p className="text-sm text-gray-600">AI analyzes key facial points (eye distance, jawline, nose width) using deep learning to recognize individuals, similar to a fingerprint but for your face.</p>
            </div>
            
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">Where It's Used</h4>
              <p className="text-sm text-gray-600">Found in security systems, phone unlocking, payment verification, law enforcement, attendance tracking, and personalized marketing.</p>
            </div>
            
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">Privacy & Ethics</h4>
              <p className="text-sm text-gray-600">While powerful, it raises concerns. Best practices include consent, transparency, security, and user control over biometric data.</p>
            </div>
          </div>
          
          <h3 className="text-lg font-medium mb-4">Real-World Uses of Facial Recognition</h3>
          <ul className="space-y-2 mb-6">
            {businessUses.map((use, index) => (
              <li key={index} className="flex items-start">
                <div className="h-5 w-5 rounded-full bg-blue-100 flex items-center justify-center text-blue-600 mr-2 mt-0.5">
                  <span className="text-xs">{index + 1}</span>
                </div>
                <div>
                  <span className="font-medium">{use.name}</span> - {use.description}
                </div>
              </li>
            ))}
          </ul>
          
          <h3 className="text-lg font-medium mb-4">Understanding Facial Recognition Accuracy</h3>
          <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">False Accept Rate (FAR)</h4>
              <p className="text-sm text-gray-600">How often the system mistakenly approves the wrong person. A lower rate means better security.</p>
            </div>
            
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">False Reject Rate (FRR)</h4>
              <p className="text-sm text-gray-600">How often the system wrongly denies an authorized user. A lower rate means better reliability.</p>
            </div>
            
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">Equal Error Rate (EER)</h4>
              <p className="text-sm text-gray-600">The balance point where FAR and FRR are equal, used to compare system performance. Lower is better.</p>
            </div>
            
            <div className="p-4 bg-white rounded-md shadow-sm">
              <h4 className="font-medium text-blue-800 mb-2">Accuracy</h4>
              <p className="text-sm text-gray-600">The overall success rate of correctly identifying people. The higher, the better!</p>
            </div>
          </div>
        </div>
        
        <h3 className="font-medium text-gray-800 mb-3">Step 2: Select from Sample Images to have AI perform facial recognition and review the results.</h3>
        
        {/* Status Indicator and Detection Mode Toggle */}
        <div className="flex flex-col sm:flex-row justify-between items-start sm:items-center mb-4">
          <div className="mb-2 sm:mb-0">
            {!isModelLoaded && useLocalDetection ? (
              <div className="p-3 bg-green-50 text-green-700 rounded-md">
                <p className="flex items-center">
                  <CheckCircle className="mr-2 h-5 w-5" />
                  Using lightweight local face detection
                </p>
              </div>
            ) : !isModelLoaded ? (
              <div className="p-3 bg-blue-50 text-blue-700 rounded-md">
                <p className="flex items-center">
                  <RefreshCw className="animate-spin mr-2 h-5 w-5" />
                  Loading facial recognition models... This may take a moment.
                </p>
              </div>
            ) : !isFaceDB ? (
              <div className="p-3 bg-yellow-50 text-yellow-700 rounded-md">
                <p className="flex items-center">
                  <AlertTriangle className="mr-2 h-5 w-5" />
                  No face database loaded. Using simulation mode.
                </p>
              </div>
            ) : null}
          </div>
          
          <button
            onClick={toggleDetectionMode}
            className={`px-3 py-2 rounded-md text-sm font-medium ${
              useLocalDetection 
                ? 'bg-green-100 text-green-800 hover:bg-green-200' 
                : 'bg-blue-100 text-blue-800 hover:bg-blue-200'
            } transition-colors`}
          >
            {useLocalDetection 
              ? 'Using Local Detection (Fast)' 
              : 'Using External Models (Accurate)'}
          </button>
        </div>
        
        {/* Registration Success Message */}
        {showRegistrationSuccess && (
          <div className="mb-4 p-3 bg-green-50 text-green-700 rounded-md flex items-center">
            <CheckCircle className="mr-2 h-5 w-5" />
            <p>Face registered successfully! You can now authenticate with your own face.</p>
          </div>
        )}
        
        {/* Display registered user info if available */}
        {registeredUserFace && registeredUserName && !registrationMode && !uploadedImage && (
          <div className="mb-6 p-4 border rounded-lg bg-blue-50">
            <h3 className="text-lg font-medium mb-2">Your Registered Face</h3>
            <div className="flex items-center">
              <div className="w-16 h-16 rounded-full overflow-hidden mr-4 border border-blue-300">
                <img src={registeredUserFace.imageUrl || ''} alt="Your face" className="w-full h-full object-cover" />
              </div>
              <div>
                <p className="font-medium">{registeredUserName}</p>
                <p className="text-sm text-gray-600">Registered User</p>
              </div>
            </div>
            
            {/* Clear registration button */}
            <div className="mt-4 text-right">
              <button
                onClick={() => {
                  // Clear the registered face data from localStorage and state
                  localStorage.removeItem('registeredUserFace');
                  localStorage.removeItem('registeredUserName');
                  setRegisteredUserFace(null);
                  setRegisteredUserName('');
                  
                  // Update face descriptors if needed
                  if (faceDescriptors.length > 0) {
                    // Remove the registered user from face descriptors
                    const updatedDescriptors = faceDescriptors.filter(face => face.name !== registeredUserName);
                    setFaceDescriptors(updatedDescriptors);
                  }
                }}
                className="px-3 py-1 bg-red-100 text-red-700 rounded-md hover:bg-red-200 transition-colors text-sm"
              >
                Clear Registration
              </button>
            </div>
          </div>
        )}
        
        {/* Upload Section */}
        <div className="mb-8">
          {!uploadedImage && !registrationMode ? (
            <div className="w-full max-w-md">
              {!usingCamera ? (
                <>
                  {/* Register Face Button - New addition */}
                  {!registeredUserFace && (
                    <div className="mb-6 w-full">
                      <h3 className="text-lg font-medium mb-3">Register Your Face</h3>
                      <div className="border-2 border-dashed border-blue-300 rounded-lg p-6 text-center bg-blue-50">
                        <p className="text-gray-600 mb-4">Register your own face to make this demo real!</p>
                        <div className="mb-4">
                          <label htmlFor="userName" className="block text-sm font-medium text-gray-700 mb-1">
                            Your Name
                          </label>
                          <input
                            type="text"
                            id="userName"
                            value={registeredUserName}
                            onChange={(e) => setRegisteredUserName(e.target.value)}
                            className="border border-gray-300 rounded-md px-3 py-2 w-full"
                            placeholder="Enter your name"
                          />
                        </div>
                        <button
                          onClick={startRegistration}
                          disabled={!registeredUserName.trim()}
                          className={`px-4 py-2 ${
                            registeredUserName.trim() ? 'bg-green-600 hover:bg-green-700' : 'bg-gray-400 cursor-not-allowed'
                          } text-white rounded-md transition-colors`}
                        >
                          Register My Face
                        </button>
                        
                        {/* Privacy Notice */}
                        <div className="mt-4 pt-3 border-t border-blue-200">
                          <p className="text-xs text-gray-600 flex items-center">
                            <Shield className="h-3 w-3 mr-1 text-blue-600" /> 
                            Your data stays on your device and is processed locally
                          </p>
                        </div>
                        
                        {/* How it works explanation */}
                        <HowItWorksModal />
                      </div>
                    </div>
                  )}

              {/* Sample Images */}
              <div className="mt-6">
                <h3 className="text-md font-medium mb-3">Select a sample image for AI analysis:</h3>
                <div className="grid grid-cols-2 md:grid-cols-5 gap-3">
                  {registeredFaces.map((face) => (
                    <button 
                      key={face.id}
                      onClick={() => handleSampleImage(face.id)}
                      className="p-3 bg-gray-100 hover:bg-gray-200 rounded border border-gray-300 text-center transition-colors"
                    >
                      {face.name}
                      <div className="text-xs text-gray-500">{face.role}</div>
                    </button>
                  ))}
                </div>
              </div>
                </>
              ) : (
                <>
                  {/* Camera Interface */}
                  <div className="text-center mb-4">
                    <h3 className="text-lg font-medium mb-2">
                      {registrationMode ? 'Register Your Face' : 'Camera Authentication'}
                    </h3>
                    <p className="text-gray-500 mb-2">Position your face inside the circle</p>
                    <p className="text-xs text-gray-500 mb-4">Your camera should activate automatically. If not, check your browser permissions.</p>
                    
                    {cameraError ? (
                      <div className="bg-red-50 p-4 rounded-md border border-red-200 mb-4">
                        <p className="text-red-600 font-medium">{cameraError}</p>
                        <p className="text-sm text-gray-600 mt-2">
                          Please check your browser settings to ensure camera access is allowed for this site.
                          You may need to click the camera icon in your address bar to grant permission.
                        </p>
                        <div className="mt-3">
                          <button 
                            onClick={() => {
                              setCameraError(null);
                              startCamera();
                            }}
                            className="px-3 py-1 bg-blue-600 text-white rounded-md hover:bg-blue-700 text-sm"
                          >
                            Try Again
                          </button>
                        </div>
                      </div>
                    ) : (
                      <>
                        {/* Make sure the ID is correctly set */}
                        <div id="camera-container" className="relative w-full max-w-md mx-auto border rounded-lg overflow-hidden">
                          <video 
                            ref={videoRef} 
                            autoPlay 
                            playsInline 
                            muted 
                            className="w-full h-auto"
                            onPlay={() => console.log("Camera video stream started playing")}
                          ></video>
                          <canvas ref={canvasRef} className="hidden"></canvas>
                          {/* Face detection overlay is added by CameraViewWithDetection component */}
                          <div className="absolute top-2 right-2 z-10">
                            <div className="bg-green-500 animate-pulse h-3 w-3 rounded-full"></div>
                          </div>
                          
                          {/* Overlay if no camera is showing */}
                          {usingCamera && !videoRef.current?.srcObject && (
                            <div className="absolute inset-0 flex items-center justify-center bg-black bg-opacity-20">
                              <div className="bg-white p-4 rounded-lg shadow text-center">
                                <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-blue-600 mx-auto mb-2"></div>
                                <p className="text-sm">Waiting for camera...</p>
                                <p className="text-xs text-gray-500 mt-1">This may take a moment</p>
                              </div>
                            </div>
                          )}
                        </div>
                        
                        {/* Add the CameraViewWithDetection component */}
                        <CameraViewWithDetection />
                        
                        {/* Camera Debug Info - for troubleshooting */}
                        <div className="mt-2 text-xs text-left text-gray-500 p-2 bg-gray-50 rounded-md">
                          <p>Camera Status: {usingCamera ? 'Active' : 'Inactive'}</p>
                          <p>Video Element: {videoRef.current ? 'Ready' : 'Not Ready'}</p>
                          <p>
                            Stream: {videoRef.current && videoRef.current.srcObject ? 'Connected' : 'Not Connected'}
                            {videoRef.current && !videoRef.current.srcObject && (
                              <span className="text-yellow-600 ml-1">â ï¸ Camera stream not connected</span>
                            )}
                          </p>
                          {videoRef.current && videoRef.current.srcObject ? (
                            <p className="text-green-600">Camera is working properly</p>
                          ) : (
                            <div className="mt-1">
                              <p className="text-yellow-600">Camera not showing? Try these steps:</p>
                              <ol className="list-decimal ml-4 mt-1 text-xs">
                                <li>Check browser permissions</li>
                                <li>Refresh the page and try again</li>
                                <li>Try a different browser</li>
                                <li>Make sure no other app is using your camera</li>
                              </ol>
                              <button 
                                onClick={() => {
                                  stopCamera();
                                  setTimeout(() => startCamera(), 500);
                                }}
                                className="px-2 py-1 bg-blue-600 text-white rounded text-xs mt-2"
                              >
                                Restart Camera
                              </button>
                            </div>
                          )}
                        </div>
                        
                        <div className="mt-4 flex justify-center space-x-4">
                          {registrationMode ? (
                            <button 
                              onClick={registerUserFace}
                              className="px-4 py-2 bg-green-600 text-white rounded-md hover:bg-green-700 transition-colors"
                              disabled={!videoRef.current?.srcObject}
                            >
                              Capture & Register
                            </button>
                          ) : (
                            <button 
                              onClick={captureAndAuthenticate}
                              className="px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition-colors"
                              disabled={!videoRef.current?.srcObject}
                            >
                              Authenticate
                            </button>
                          )}
                          <button 
                            onClick={registrationMode ? cancelRegistration : stopCamera}
                            className="px-4 py-2 bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition-colors"
                          >
                            Cancel
                          </button>
                        </div>
                        
                        {/* Privacy Notice */}
                        <PrivacyNotice />
                      </>
                    )}
                  </div>
                </>
              )}
            </div>
          ) : (
            <div className="w-full max-w-md">
              {/* Image Preview */}
              <div className="mb-6">
                <h3 className="text-lg font-medium mb-3">Image Preview</h3>
                <div className="border rounded-lg overflow-hidden">
                  <img 
                    src={uploadedImage || ''} 
                    alt="Face to recognize" 
                    className="w-full h-auto max-h-64 object-contain" 
                  />
                </div>
              </div>
              
              {/* Advanced Options */}
              <div className="mb-6">
                <button 
                  onClick={() => setShowAdvancedOptions(!showAdvancedOptions)}
                  className="text-blue-600 hover:text-blue-800 text-sm mb-2 flex items-center"
                >
                  {showAdvancedOptions ? 'Hide' : 'Show'} Advanced Options
                </button>
                
                {showAdvancedOptions && (
                  <div className="bg-gray-50 p-4 rounded-lg">
                    <div className="mb-4">
                      <label className="block text-sm font-medium text-gray-700 mb-1">
                        Confidence Threshold: {confidenceThreshold.toFixed(2)}
                      </label>
                      <input 
                        type="range" 
                        min="0.7" 
                        max="0.99" 
                        step="0.01"
                        value={confidenceThreshold}
                        onChange={(e) => setConfidenceThreshold(parseFloat(e.target.value))}
                        className="w-full"
                      />
                      <p className="text-xs text-gray-500 mt-1">Higher threshold = fewer false positives, more false negatives</p>
                    </div>
                    
                    <div>
                      <label className="flex items-center">
                        <input 
                          type="checkbox" 
                          checked={isAuthorizedOnly} 
                          onChange={() => setIsAuthorizedOnly(!isAuthorizedOnly)}
                          className="mr-2" 
                        />
                        <span className="text-sm font-medium text-gray-700">Only allow authorized personnel</span>
                      </label>
                    </div>
                  </div>
                )}
              </div>
              
              <div className="flex space-x-4">
                <button 
                  onClick={processImage}
                  className="flex-1 px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition-colors"
                  disabled={!isModelLoaded}
                >
                  {isModelLoaded ? 'Recognize Face' : 'Loading Models...'}
                </button>
                <button 
                  onClick={resetDemo}
                  className="px-4 py-2 bg-gray-200 text-gray-800 rounded-md hover:bg-gray-300 transition-colors"
                >
                  Reset
                </button>
              </div>
            </div>
          )}
        </div>
        
        {/* Processing Section */}
        {demoStage === 'processing' && (
          <div className="text-center py-10">
            <div className="animate-spin rounded-full h-16 w-16 border-b-2 border-blue-600 mx-auto mb-4"></div>
            <h3 className="text-lg font-medium">Processing Image...</h3>
            <p className="text-gray-500">Detecting and analyzing faces</p>
          </div>
        )}
        
        {/* Results Section */}
        {demoStage === 'result' && (
          <div className="w-full max-w-md">
            <div className="mb-6">
              <div className="border rounded-lg overflow-hidden">
                <img 
                  src={uploadedImage || ''} 
                  alt="Face to recognize" 
                  className="w-full h-auto max-h-64 object-contain" 
                />
              </div>
            </div>
            
            <div className="mb-6">
              <h3 className="text-lg font-medium mb-4">Facial Recognition Results</h3>
              
              {recognitionResult ? (
                <div className="border rounded-lg p-4 bg-green-50 border-green-200">
                  <div className="flex items-center mb-4">
                    <CheckCircle size={24} className="text-green-600 mr-2" />
                    <span className="text-lg font-medium text-green-800">Face Recognized!</span>
                  </div>
                  
                  <div className="grid grid-cols-2 gap-4">
                    <div>
                      <p className="text-sm text-gray-500">Name</p>
                      <p className="font-medium">{recognitionResult.name}</p>
                    </div>
                    <div>
                      <p className="text-sm text-gray-500">Role</p>
                      <p className="font-medium">{recognitionResult.role}</p>
                    </div>
                    <div>
                      <p className="text-sm text-gray-500">Confidence</p>
                      <p className="font-medium">{(recognitionResult.confidence * 100).toFixed(1)}%</p>
                    </div>
                    <div>
                      <p className="text-sm text-gray-500">Status</p>
                      <p className={`font-medium ${recognitionResult.authorized ? 'text-green-600' : 'text-red-600'}`}>
                        {recognitionResult.authorized ? 'Authorized' : 'Unauthorized'}
                      </p>
                    </div>
                    <div className="col-span-2">
                      <p className="text-sm text-gray-500">Last Seen</p>
                      <p className="font-medium">{recognitionResult.lastSeen}</p>
                    </div>
                  </div>
                </div>
              ) : (
                <div className="border rounded-lg p-4 bg-red-50 border-red-200">
                  <div className="flex items-center mb-4">
                    <XCircle size={24} className="text-red-600 mr-2" />
                    <span className="text-lg font-medium text-red-800">No Match Found</span>
                  </div>
                  <p className="text-gray-700">
                    The face could not be recognized or does not meet the confidence threshold ({confidenceThreshold.toFixed(2)}).
                    {isAuthorizedOnly && " Additionally, we're only showing authorized personnel."}
                  </p>
                </div>
              )}
            </div>
            
            <div className="flex space-x-4">
              <button 
                onClick={() => setDemoStage('upload')}
                className="flex-1 px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700 transition-colors"
              >
                Try Another Sample
              </button>
              <button 
                onClick={handleCompleteChallenge}
                className="px-4 py-2 bg-green-600 text-white rounded-md hover:bg-green-700 transition-colors"
              >
                Challenge Completed
              </button>
            </div>
          </div>
        )}
        
        {/* Environmental Factors Section */}
        <div className="mt-10 pt-6 border-t border-gray-200">
          <h2 className="text-xl font-bold mb-4">Environmental Factors</h2>
          <p className="text-gray-600 mb-6">
            Various environmental factors can affect face recognition accuracy. Here's what to consider:
          </p>
          
          <div className="grid grid-cols-1 md:grid-cols-3 gap-6">
            {environmentalFactors.map((factor, index) => (
              <div key={index} className="border rounded-lg p-4 hover:shadow-md transition-shadow">
                <div className="flex items-center mb-2">
                  <div className="bg-blue-100 p-2 rounded-full mr-3 text-blue-600">
                    {factor.icon}
                  </div>
                  <h3 className="font-medium">{factor.name}</h3>
                </div>
                <p className="text-gray-600 text-sm">{factor.description}</p>
              </div>
            ))}
          </div>
        </div>
        
        {/* Business Applications */}
        <div className="mt-10 pt-6 border-t border-gray-200">
          <h2 className="text-xl font-bold mb-4">Business Applications</h2>
          <ul className="space-y-4">
            {businessUses.map((use, index) => (
              <li key={index} className="border-b border-gray-100 pb-3 last:border-b-0 last:pb-0">
                <h3 className="font-medium text-blue-800">{use.name}</h3>
                <p className="text-gray-600 text-sm">{use.description}</p>
              </li>
            ))}
          </ul>
        </div>
        
        {/* Privacy & Security - Fixed styling */}
        <div className="mt-10 pt-6 border-t border-gray-200 mb-6">
          <h2 className="text-xl font-bold mb-4">Privacy & Ethical Considerations</h2>
          <div className="mb-6">
            <ul className="space-y-3 mb-6">
              {privacyConsiderations.map((item, index) => (
                <li key={index} className="flex">
                  <Shield size={18} className="mr-2 text-blue-600 flex-shrink-0 mt-0.5" />
                  <div>
                    <span className="font-medium block">{item.name}</span>
                    <span className="text-gray-600 text-sm">{item.description}</span>
                  </div>
                </li>
              ))}
            </ul>
          </div>
        </div>
      </div>
    </div>
  )
}

export default FaceIdManagerMain 